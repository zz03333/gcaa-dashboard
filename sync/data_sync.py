#!/usr/bin/env python3
"""
GCAA ç¤¾ç¾¤åˆ†æ - è³‡æ–™åŒæ­¥è…³æœ¬
å¾ Google Sheets è®€å– raw_posts + raw_post_insightsï¼Œç”Ÿæˆ JSON æª”æ¡ˆä¾›å‰ç«¯ä½¿ç”¨
"""

import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
from google.oauth2 import service_account
from googleapiclient.discovery import build

# è¨­å®š
SPREADSHEET_ID = '1HJXQrlB0eYJsHmioLMNfCKV_OXHqqgwtwRtO9s5qbB0'
SERVICE_ACCOUNT_FILE = os.path.join(os.path.dirname(__file__), '..', 'esg-reports-collection-9661012923ed.json')
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', 'public', 'data')

# Sheets è¨­å®š - ç¾åœ¨åªä½¿ç”¨ raw_post_insights (å·²æ•´åˆæ‰€æœ‰è²¼æ–‡è³‡æ–™)
SHEETS = {
    'raw_insights': 'raw_post_insights'
}

def get_sheets_service():
    """å»ºç«‹ Google Sheets API é€£ç·š"""
    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE,
        scopes=['https://www.googleapis.com/auth/spreadsheets.readonly']
    )
    return build('sheets', 'v4', credentials=credentials)

def fetch_sheet_data(service, sheet_name):
    """å¾ Google Sheets è®€å–è³‡æ–™"""
    result = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=sheet_name
    ).execute()

    values = result.get('values', [])
    if len(values) < 2:
        return []

    headers = values[0]
    data = []
    for row in values[1:]:
        obj = {}
        for i, header in enumerate(headers):
            obj[header] = row[i] if i < len(row) else ''
        data.append(obj)

    return data

def parse_datetime(date_str):
    """è§£ææ—¥æœŸæ™‚é–“å­—ä¸²"""
    if not date_str:
        return None
    try:
        # å˜—è©¦å¤šç¨®æ ¼å¼
        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%Y-%m-%d', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        return None
    except Exception:
        return None

def parse_int(value):
    """å®‰å…¨è§£ææ•´æ•¸"""
    if not value:
        return 0
    try:
        # ç§»é™¤é€—è™Ÿ (ä¾‹å¦‚ "52,143" -> 52143)
        return int(str(value).replace(',', '').replace(' ', ''))
    except (ValueError, TypeError):
        return 0

def parse_float(value):
    """å®‰å…¨è§£ææµ®é»æ•¸"""
    if not value:
        return 0.0
    try:
        return float(str(value).replace(',', '').replace(' ', ''))
    except (ValueError, TypeError):
        return 0.0

def process_insights_data(raw_insights):
    """è™•ç† raw_post_insights è³‡æ–™ï¼ˆå·²æ•´åˆæ‰€æœ‰è²¼æ–‡è³‡è¨Šï¼‰"""
    posts = []
    for row in raw_insights:
        post_id = row.get('Post ID', '')
        if not post_id:
            continue

        # åŸºæœ¬è³‡è¨Š (ç¾åœ¨éƒ½åœ¨ raw_post_insights ä¸­)
        published_at = parse_datetime(row.get('ç™¼å¸ƒæ™‚é–“ (GMT+8)', ''))
        content = row.get('å…§å®¹é è¦½', '') or ''

        # äº’å‹•æŒ‡æ¨™
        likes = parse_int(row.get('ç¸½è®šæ•¸', 0))
        comments = parse_int(row.get('ç•™è¨€æ•¸', 0))
        shares = parse_int(row.get('åˆ†äº«æ•¸', 0))
        clicks = parse_int(row.get('é»æ“Šæ•¸', 0))
        reach = parse_int(row.get('è§¸åŠäººæ•¸', 0))
        video_views = parse_int(row.get('å½±ç‰‡è§€çœ‹', 0))

        # è¡¨æƒ…åæ‡‰
        reactions = {
            'like': parse_int(row.get('ğŸ‘åæ‡‰', 0)),
            'love': parse_int(row.get('â¤ï¸åæ‡‰', 0)),
            'wow': parse_int(row.get('ğŸ˜®åæ‡‰', 0)),
            'haha': parse_int(row.get('ğŸ˜†åæ‡‰', 0)),
            'sad': parse_int(row.get('ğŸ˜¢åæ‡‰', 0)),
            'angry': parse_int(row.get('ğŸ˜ åæ‡‰', 0))
        }

        # è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
        total_engagement = likes + comments + shares
        engagement_rate = (total_engagement / reach * 100) if reach > 0 else 0
        share_rate = (shares / reach * 100) if reach > 0 else 0

        # å»£å‘Šè³‡è¨Š
        is_promoted = row.get('æœ‰æŠ•å»£', 'å¦') == 'æ˜¯'
        ad_status = row.get('å»£å‘Šç‹€æ…‹', '')
        ad_spend = parse_float(row.get('å»£å‘ŠèŠ±è²»', 0))

        posts.append({
            'id': post_id,
            'publishedAt': published_at.isoformat() if published_at else None,
            'content': content,
            'contentPreview': content[:80] + '...' if len(content) > 80 else content,
            'actionType': row.get('è¡Œå‹•é¡å‹', '') or 'å…¶ä»–',
            'topic': row.get('è­°é¡Œé¡å‹', '') or 'å…¶ä»–',
            'permalink': row.get('è²¼æ–‡é€£çµ', ''),
            'isPromoted': is_promoted,
            'adStatus': ad_status,
            'adSpend': ad_spend,
            'metrics': {
                'likes': likes,
                'comments': comments,
                'shares': shares,
                'clicks': clicks,
                'reach': reach,
                'videoViews': video_views,
                'reactions': reactions
            },
            'computed': {
                'engagementRate': round(engagement_rate, 2),
                'totalEngagement': total_engagement,
                'shareRate': round(share_rate, 2)
            }
        })

    # æŒ‰ç™¼å¸ƒæ™‚é–“æ’åº (æ–°åˆ°èˆŠ)
    posts.sort(key=lambda x: x['publishedAt'] or '', reverse=True)

    return posts

def generate_daily_data(posts):
    """ç”Ÿæˆæ¯æ—¥èšåˆè³‡æ–™"""
    daily_map = defaultdict(lambda: {
        'postCount': 0,
        'totalReach': 0,
        'totalEngagement': 0,
        'totalShares': 0,
        'totalClicks': 0,
        'engagementRates': []
    })

    for post in posts:
        if not post['publishedAt']:
            continue

        date = post['publishedAt'][:10]  # YYYY-MM-DD
        daily = daily_map[date]

        daily['postCount'] += 1
        daily['totalReach'] += post['metrics']['reach']
        daily['totalEngagement'] += post['computed']['totalEngagement']
        daily['totalShares'] += post['metrics']['shares']
        daily['totalClicks'] += post['metrics']['clicks']
        if post['computed']['engagementRate'] > 0:
            daily['engagementRates'].append(post['computed']['engagementRate'])

    daily_data = []
    for date, data in sorted(daily_map.items(), reverse=True):
        avg_er = sum(data['engagementRates']) / len(data['engagementRates']) if data['engagementRates'] else 0
        daily_data.append({
            'date': date,
            'postCount': data['postCount'],
            'totalReach': data['totalReach'],
            'totalEngagement': data['totalEngagement'],
            'avgEngagementRate': round(avg_er, 2),
            'totalShares': data['totalShares'],
            'totalClicks': data['totalClicks']
        })

    return daily_data

def generate_stats(posts):
    """ç”Ÿæˆçµ±è¨ˆæ‘˜è¦"""
    # æŒ‰è¡Œå‹•é¡å‹åˆ†çµ„
    by_action = defaultdict(lambda: {'count': 0, 'totalER': 0, 'totalReach': 0})
    for post in posts:
        action = post['actionType']
        by_action[action]['count'] += 1
        by_action[action]['totalER'] += post['computed']['engagementRate']
        by_action[action]['totalReach'] += post['metrics']['reach']

    action_stats = []
    for name, data in sorted(by_action.items(), key=lambda x: -x[1]['count']):
        action_stats.append({
            'name': name,
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0,
            'avgReach': round(data['totalReach'] / data['count']) if data['count'] > 0 else 0
        })

    # æŒ‰è­°é¡Œåˆ†çµ„
    by_topic = defaultdict(lambda: {'count': 0, 'totalER': 0, 'totalReach': 0})
    for post in posts:
        topic = post['topic']
        by_topic[topic]['count'] += 1
        by_topic[topic]['totalER'] += post['computed']['engagementRate']
        by_topic[topic]['totalReach'] += post['metrics']['reach']

    topic_stats = []
    for name, data in sorted(by_topic.items(), key=lambda x: -x[1]['count']):
        topic_stats.append({
            'name': name,
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0,
            'avgReach': round(data['totalReach'] / data['count']) if data['count'] > 0 else 0
        })

    # æŒ‰å°æ™‚åˆ†çµ„
    by_hour = defaultdict(lambda: {'count': 0, 'totalER': 0})
    for post in posts:
        if not post['publishedAt']:
            continue
        try:
            hour = int(post['publishedAt'][11:13])
            by_hour[hour]['count'] += 1
            by_hour[hour]['totalER'] += post['computed']['engagementRate']
        except (ValueError, IndexError):
            continue

    hour_stats = []
    for hour in range(24):
        data = by_hour[hour]
        hour_stats.append({
            'hour': hour,
            'label': f'{hour:02d}:00',
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0
        })

    # æŒ‰æ˜ŸæœŸåˆ†çµ„
    weekday_names = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']
    by_weekday = defaultdict(lambda: {'count': 0, 'totalER': 0})
    for post in posts:
        if not post['publishedAt']:
            continue
        try:
            dt = datetime.fromisoformat(post['publishedAt'])
            weekday = dt.weekday()
            by_weekday[weekday]['count'] += 1
            by_weekday[weekday]['totalER'] += post['computed']['engagementRate']
        except (ValueError, TypeError):
            continue

    weekday_stats = []
    for i in range(7):
        data = by_weekday[i]
        weekday_stats.append({
            'weekday': i,
            'name': weekday_names[i],
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0
        })

    # æ™‚æ®µç†±åŠ›åœ– (æ˜ŸæœŸ x å°æ™‚)
    heatmap = []
    by_weekday_hour = defaultdict(lambda: {'count': 0, 'totalER': 0})
    for post in posts:
        if not post['publishedAt']:
            continue
        try:
            dt = datetime.fromisoformat(post['publishedAt'])
            key = (dt.weekday(), dt.hour)
            by_weekday_hour[key]['count'] += 1
            by_weekday_hour[key]['totalER'] += post['computed']['engagementRate']
        except (ValueError, TypeError):
            continue

    for weekday in range(7):
        for hour in range(24):
            data = by_weekday_hour[(weekday, hour)]
            heatmap.append({
                'weekday': weekday,
                'weekdayName': weekday_names[weekday],
                'hour': hour,
                'count': data['count'],
                'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0
            })

    return {
        'lastUpdated': datetime.now().isoformat(),
        'totalPosts': len(posts),
        'byActionType': action_stats,
        'byTopic': topic_stats,
        'byHour': hour_stats,
        'byDayOfWeek': weekday_stats,
        'heatmap': heatmap
    }

def main():
    print('GCAA ç¤¾ç¾¤åˆ†æ - è³‡æ–™åŒæ­¥é–‹å§‹')
    print(f'Service Account: {SERVICE_ACCOUNT_FILE}')
    print(f'Spreadsheet ID: {SPREADSHEET_ID}')

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # é€£æ¥ Google Sheets
    print('\né€£æ¥ Google Sheets...')
    service = get_sheets_service()

    # è®€å–è³‡æ–™ (åªéœ€è¦ raw_post_insightsï¼Œå·²æ•´åˆæ‰€æœ‰è³‡è¨Š)
    print('è®€å– raw_post_insights...')
    raw_insights = fetch_sheet_data(service, SHEETS['raw_insights'])
    print(f'  - {len(raw_insights)} ç­†è²¼æ–‡')

    # è™•ç†è³‡æ–™
    print('\nè™•ç†è³‡æ–™...')
    posts = process_insights_data(raw_insights)
    print(f'  - è™•ç†å¾Œ: {len(posts)} ç­†è²¼æ–‡')

    # ç”Ÿæˆèšåˆè³‡æ–™
    daily = generate_daily_data(posts)
    print(f'  - æ¯æ—¥è³‡æ–™: {len(daily)} å¤©')

    stats = generate_stats(posts)
    print(f'  - è¡Œå‹•é¡å‹: {len(stats["byActionType"])} ç¨®')
    print(f'  - è­°é¡Œ: {len(stats["byTopic"])} ç¨®')

    # å¯«å…¥ JSON æª”æ¡ˆ
    print('\nå¯«å…¥ JSON æª”æ¡ˆ...')

    posts_file = os.path.join(OUTPUT_DIR, 'posts.json')
    with open(posts_file, 'w', encoding='utf-8') as f:
        json.dump(posts, f, ensure_ascii=False, indent=2)
    print(f'  - {posts_file}')

    daily_file = os.path.join(OUTPUT_DIR, 'daily.json')
    with open(daily_file, 'w', encoding='utf-8') as f:
        json.dump(daily, f, ensure_ascii=False, indent=2)
    print(f'  - {daily_file}')

    stats_file = os.path.join(OUTPUT_DIR, 'stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f'  - {stats_file}')

    print('\nåŒæ­¥å®Œæˆ!')
    print(f'è³‡æ–™æ›´æ–°æ™‚é–“: {stats["lastUpdated"]}')

if __name__ == '__main__':
    main()
