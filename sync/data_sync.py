#!/usr/bin/env python3
"""
GCAA ç¤¾ç¾¤åˆ†æ - è³‡æ–™åŒæ­¥è…³æœ¬
å¾ Google Sheets è®€å– raw_posts + raw_post_insightsï¼Œç”Ÿæˆ JSON æª”æ¡ˆä¾›å‰ç«¯ä½¿ç”¨
"""

import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
from google.oauth2 import service_account
from googleapiclient.discovery import build

# è¨­å®š
SPREADSHEET_ID = '1HJXQrlB0eYJsHmioLMNfCKV_OXHqqgwtwRtO9s5qbB0'
SERVICE_ACCOUNT_FILE = os.path.join(os.path.dirname(__file__), '..', 'esg-reports-collection-9661012923ed.json')
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', 'public', 'data')

# Sheets è¨­å®š
SHEETS = {
    'raw_insights': 'raw_post_insights',
    'content_analysis': 'ğŸ“Š content_analysis',
    'posts_performance': 'ğŸ“ˆ posts_performance',
    'ad_analytics': 'ğŸ’° ad_analytics'
}

def get_sheets_service():
    """å»ºç«‹ Google Sheets API é€£ç·š"""
    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE,
        scopes=['https://www.googleapis.com/auth/spreadsheets.readonly']
    )
    return build('sheets', 'v4', credentials=credentials)

def fetch_sheet_data(service, sheet_name):
    """å¾ Google Sheets è®€å–è³‡æ–™ (ä½¿ç”¨ header æ¨¡å¼)"""
    result = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=sheet_name
    ).execute()

    values = result.get('values', [])
    if len(values) < 2:
        return []

    headers = values[0]
    data = []
    for row in values[1:]:
        obj = {}
        for i, header in enumerate(headers):
            obj[header] = row[i] if i < len(row) else ''
        data.append(obj)

    return data


def fetch_sheet_raw(service, sheet_name):
    """å¾ Google Sheets è®€å–åŸå§‹è³‡æ–™ (ä¸ä½¿ç”¨ header æ¨¡å¼)"""
    result = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=sheet_name
    ).execute()

    values = result.get('values', [])
    return values  # Return raw 2D array

def parse_datetime(date_str):
    """è§£ææ—¥æœŸæ™‚é–“å­—ä¸²"""
    if not date_str:
        return None
    try:
        # å˜—è©¦å¤šç¨®æ ¼å¼
        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%Y-%m-%d', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        return None
    except Exception:
        return None

def parse_int(value):
    """å®‰å…¨è§£ææ•´æ•¸"""
    if not value:
        return 0
    try:
        # ç§»é™¤é€—è™Ÿ (ä¾‹å¦‚ "52,143" -> 52143)
        return int(str(value).replace(',', '').replace(' ', ''))
    except (ValueError, TypeError):
        return 0

def parse_float(value):
    """å®‰å…¨è§£ææµ®é»æ•¸"""
    if not value:
        return 0.0
    try:
        return float(str(value).replace(',', '').replace(' ', ''))
    except (ValueError, TypeError):
        return 0.0

def process_insights_data(raw_insights):
    """è™•ç† raw_post_insights è³‡æ–™ï¼ˆå·²æ•´åˆæ‰€æœ‰è²¼æ–‡è³‡è¨Šï¼‰"""
    posts = []
    for row in raw_insights:
        post_id = row.get('Post ID', '')
        if not post_id:
            continue

        # åŸºæœ¬è³‡è¨Š (ç¾åœ¨éƒ½åœ¨ raw_post_insights ä¸­)
        published_at = parse_datetime(row.get('ç™¼å¸ƒæ™‚é–“ (GMT+8)', ''))
        content = row.get('å…§å®¹é è¦½', '') or ''

        # äº’å‹•æŒ‡æ¨™
        likes = parse_int(row.get('ç¸½è®šæ•¸', 0))
        comments = parse_int(row.get('ç•™è¨€æ•¸', 0))
        shares = parse_int(row.get('åˆ†äº«æ•¸', 0))
        clicks = parse_int(row.get('é»æ“Šæ•¸', 0))
        reach = parse_int(row.get('è§¸åŠäººæ•¸', 0))
        video_views = parse_int(row.get('å½±ç‰‡è§€çœ‹', 0))

        # è¡¨æƒ…åæ‡‰
        reactions = {
            'like': parse_int(row.get('ğŸ‘åæ‡‰', 0)),
            'love': parse_int(row.get('â¤ï¸åæ‡‰', 0)),
            'wow': parse_int(row.get('ğŸ˜®åæ‡‰', 0)),
            'haha': parse_int(row.get('ğŸ˜†åæ‡‰', 0)),
            'sad': parse_int(row.get('ğŸ˜¢åæ‡‰', 0)),
            'angry': parse_int(row.get('ğŸ˜ åæ‡‰', 0))
        }

        # è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
        total_engagement = likes + comments + shares
        engagement_rate = (total_engagement / reach * 100) if reach > 0 else 0
        share_rate = (shares / reach * 100) if reach > 0 else 0

        # å»£å‘Šè³‡è¨Š
        is_promoted = row.get('æœ‰æŠ•å»£', 'å¦') == 'æ˜¯'
        ad_status = row.get('å»£å‘Šç‹€æ…‹', '')
        ad_spend = parse_float(row.get('å»£å‘ŠèŠ±è²»', 0))

        posts.append({
            'id': post_id,
            'publishedAt': published_at.isoformat() if published_at else None,
            'content': content,
            'contentPreview': content[:80] + '...' if len(content) > 80 else content,
            'actionType': row.get('è¡Œå‹•é¡å‹', '') or 'å…¶ä»–',
            'topic': row.get('è­°é¡Œé¡å‹', '') or 'å…¶ä»–',
            'permalink': row.get('è²¼æ–‡é€£çµ', ''),
            'isPromoted': is_promoted,
            'adStatus': ad_status,
            'adSpend': ad_spend,
            'metrics': {
                'likes': likes,
                'comments': comments,
                'shares': shares,
                'clicks': clicks,
                'reach': reach,
                'videoViews': video_views,
                'reactions': reactions
            },
            'computed': {
                'engagementRate': round(engagement_rate, 2),
                'totalEngagement': total_engagement,
                'shareRate': round(share_rate, 2)
            }
        })

    # æŒ‰ç™¼å¸ƒæ™‚é–“æ’åº (æ–°åˆ°èˆŠ)
    posts.sort(key=lambda x: x['publishedAt'] or '', reverse=True)

    return posts

def generate_daily_data(posts):
    """ç”Ÿæˆæ¯æ—¥èšåˆè³‡æ–™"""
    daily_map = defaultdict(lambda: {
        'postCount': 0,
        'totalReach': 0,
        'totalEngagement': 0,
        'totalShares': 0,
        'totalClicks': 0,
        'engagementRates': []
    })

    for post in posts:
        if not post['publishedAt']:
            continue

        date = post['publishedAt'][:10]  # YYYY-MM-DD
        daily = daily_map[date]

        daily['postCount'] += 1
        daily['totalReach'] += post['metrics']['reach']
        daily['totalEngagement'] += post['computed']['totalEngagement']
        daily['totalShares'] += post['metrics']['shares']
        daily['totalClicks'] += post['metrics']['clicks']
        if post['computed']['engagementRate'] > 0:
            daily['engagementRates'].append(post['computed']['engagementRate'])

    daily_data = []
    for date, data in sorted(daily_map.items(), reverse=True):
        avg_er = sum(data['engagementRates']) / len(data['engagementRates']) if data['engagementRates'] else 0
        daily_data.append({
            'date': date,
            'postCount': data['postCount'],
            'totalReach': data['totalReach'],
            'totalEngagement': data['totalEngagement'],
            'avgEngagementRate': round(avg_er, 2),
            'totalShares': data['totalShares'],
            'totalClicks': data['totalClicks']
        })

    return daily_data

def generate_stats(posts):
    """ç”Ÿæˆçµ±è¨ˆæ‘˜è¦"""
    # æŒ‰è¡Œå‹•é¡å‹åˆ†çµ„
    by_action = defaultdict(lambda: {'count': 0, 'totalER': 0, 'totalReach': 0})
    for post in posts:
        action = post['actionType']
        by_action[action]['count'] += 1
        by_action[action]['totalER'] += post['computed']['engagementRate']
        by_action[action]['totalReach'] += post['metrics']['reach']

    action_stats = []
    for name, data in sorted(by_action.items(), key=lambda x: -x[1]['count']):
        action_stats.append({
            'name': name,
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0,
            'avgReach': round(data['totalReach'] / data['count']) if data['count'] > 0 else 0
        })

    # æŒ‰è­°é¡Œåˆ†çµ„
    by_topic = defaultdict(lambda: {'count': 0, 'totalER': 0, 'totalReach': 0})
    for post in posts:
        topic = post['topic']
        by_topic[topic]['count'] += 1
        by_topic[topic]['totalER'] += post['computed']['engagementRate']
        by_topic[topic]['totalReach'] += post['metrics']['reach']

    topic_stats = []
    for name, data in sorted(by_topic.items(), key=lambda x: -x[1]['count']):
        topic_stats.append({
            'name': name,
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0,
            'avgReach': round(data['totalReach'] / data['count']) if data['count'] > 0 else 0
        })

    # æŒ‰å°æ™‚åˆ†çµ„
    by_hour = defaultdict(lambda: {'count': 0, 'totalER': 0})
    for post in posts:
        if not post['publishedAt']:
            continue
        try:
            hour = int(post['publishedAt'][11:13])
            by_hour[hour]['count'] += 1
            by_hour[hour]['totalER'] += post['computed']['engagementRate']
        except (ValueError, IndexError):
            continue

    hour_stats = []
    for hour in range(24):
        data = by_hour[hour]
        hour_stats.append({
            'hour': hour,
            'label': f'{hour:02d}:00',
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0
        })

    # æŒ‰æ˜ŸæœŸåˆ†çµ„
    weekday_names = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']
    by_weekday = defaultdict(lambda: {'count': 0, 'totalER': 0})
    for post in posts:
        if not post['publishedAt']:
            continue
        try:
            dt = datetime.fromisoformat(post['publishedAt'])
            weekday = dt.weekday()
            by_weekday[weekday]['count'] += 1
            by_weekday[weekday]['totalER'] += post['computed']['engagementRate']
        except (ValueError, TypeError):
            continue

    weekday_stats = []
    for i in range(7):
        data = by_weekday[i]
        weekday_stats.append({
            'weekday': i,
            'name': weekday_names[i],
            'count': data['count'],
            'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0
        })

    # æ™‚æ®µç†±åŠ›åœ– (æ˜ŸæœŸ x å°æ™‚)
    heatmap = []
    by_weekday_hour = defaultdict(lambda: {'count': 0, 'totalER': 0})
    for post in posts:
        if not post['publishedAt']:
            continue
        try:
            dt = datetime.fromisoformat(post['publishedAt'])
            key = (dt.weekday(), dt.hour)
            by_weekday_hour[key]['count'] += 1
            by_weekday_hour[key]['totalER'] += post['computed']['engagementRate']
        except (ValueError, TypeError):
            continue

    for weekday in range(7):
        for hour in range(24):
            data = by_weekday_hour[(weekday, hour)]
            heatmap.append({
                'weekday': weekday,
                'weekdayName': weekday_names[weekday],
                'hour': hour,
                'count': data['count'],
                'avgER': round(data['totalER'] / data['count'], 2) if data['count'] > 0 else 0
            })

    return {
        'lastUpdated': datetime.now().isoformat(),
        'totalPosts': len(posts),
        'byActionType': action_stats,
        'byTopic': topic_stats,
        'byHour': hour_stats,
        'byDayOfWeek': weekday_stats,
        'heatmap': heatmap
    }


def parse_section_data(rows, section_marker):
    """
    Parse a section from the sheet data.
    Returns data rows starting after the header row.
    """
    data = []
    in_section = False

    for row in rows:
        # Check for section marker
        first_cell = row.get(list(row.keys())[0] if row else '', '') if row else ''

        if section_marker in str(first_cell):
            in_section = True
            continue

        # Skip empty rows or other section markers
        if in_section:
            if not any(row.values()):
                continue
            # Check if this is another section marker (starts with emoji)
            if first_cell and (first_cell.startswith('ğŸ“Œ') or first_cell.startswith('ğŸ“Š') or
                              first_cell.startswith('ğŸ”¥') or first_cell.startswith('âš–ï¸') or
                              first_cell.startswith('ğŸ†') or first_cell.startswith('ğŸ“ˆ') or
                              first_cell.startswith('ğŸ’°')):
                break
            data.append(row)

    return data


def process_content_analysis(raw_rows):
    """è™•ç† content_analysis è³‡æ–™ (raw 2D array)"""
    by_action_type = []
    by_topic = []
    cross_analysis = []

    current_section = None

    for row in raw_rows:
        if not row:
            continue

        first_val = str(row[0]) if row else ''

        # Detect section headers
        if 'è¡Œå‹•é¡å‹è¡¨ç¾' in first_val:
            current_section = 'action'
            continue
        elif 'è­°é¡Œè¡¨ç¾' in first_val:
            current_section = 'topic'
            continue
        elif 'äº¤å‰åˆ†æ' in first_val:
            current_section = 'cross'
            continue

        # Skip header rows (they contain column names like 'è²¼æ–‡æ•¸')
        if 'è²¼æ–‡æ•¸' in first_val or first_val == 'è¡Œå‹•é¡å‹' or first_val == 'è­°é¡Œ' or first_val == 'è¡Œå‹•':
            continue

        # Skip empty rows
        if not first_val or first_val.strip() == '':
            continue

        # Parse data based on section
        if current_section == 'action' and len(row) >= 7:
            by_action_type.append({
                'actionType': str(row[0]),
                'postCount': parse_int(row[1]),
                'avgER': parse_float(row[2]),
                'avgShareRate': parse_float(row[3]),
                'avgCommentRate': parse_float(row[4]),
                'viralCount': parse_int(row[5]),
                'highCount': parse_int(row[6])
            })
        elif current_section == 'topic' and len(row) >= 7:
            by_topic.append({
                'topic': str(row[0]),
                'postCount': parse_int(row[1]),
                'avgER': parse_float(row[2]),
                'avgShareRate': parse_float(row[3]),
                'avgCommentRate': parse_float(row[4]),
                'viralCount': parse_int(row[5]),
                'highCount': parse_int(row[6])
            })
        elif current_section == 'cross' and len(row) >= 6:
            cross_analysis.append({
                'actionType': str(row[0]),
                'topic': str(row[1]),
                'postCount': parse_int(row[2]),
                'avgER': parse_float(row[3]),
                'avgShareRate': parse_float(row[4]),
                'highPerformerCount': parse_int(row[5])
            })

    return {
        'byActionType': by_action_type,
        'byTopic': by_topic,
        'crossAnalysis': cross_analysis
    }


def process_posts_performance(raw_rows):
    """è™•ç† posts_performance è³‡æ–™ (raw 2D array)"""
    top_posts = []
    quadrant_analysis = []
    weekly_trends = []

    current_section = None

    for row in raw_rows:
        if not row:
            continue

        first_val = str(row[0]) if row else ''

        # Detect section headers
        if 'Top' in first_val or 'è²¼æ–‡æ’è¡Œ' in first_val:
            current_section = 'top'
            continue
        elif 'è±¡é™' in first_val:
            current_section = 'quadrant'
            continue
        elif 'é€±åº¦è¶¨å‹¢' in first_val or 'é€±è¶¨å‹¢' in first_val:
            current_section = 'weekly'
            continue

        # Skip header rows
        if 'è²¼æ–‡ ID' in first_val or 'é€±æ¬¡' in first_val:
            continue

        # Skip empty rows
        if not first_val or first_val.strip() == '':
            continue

        # Parse data based on section
        if current_section == 'top' and len(row) >= 12:
            top_posts.append({
                'postId': str(row[0]),
                'contentPreview': str(row[1]) if len(row) > 1 else '',
                'publishedAt': str(row[2]) if len(row) > 2 else '',
                'actionType': str(row[3]) if len(row) > 3 else '',
                'topic': str(row[4]) if len(row) > 4 else '',
                'timeSlot': str(row[5]) if len(row) > 5 else '',
                'engagementRate': parse_float(row[6]) if len(row) > 6 else 0,
                'performanceTier': str(row[7]) if len(row) > 7 else '',
                'percentileRank': parse_float(row[8]) if len(row) > 8 else 0,
                'reach': parse_int(row[9]) if len(row) > 9 else 0,
                'totalEngagement': parse_int(row[10]) if len(row) > 10 else 0,
                'permalink': str(row[11]) if len(row) > 11 else ''
            })
        elif current_section == 'quadrant' and len(row) >= 11:
            quadrant_analysis.append({
                'postId': str(row[0]),
                'publishedAt': str(row[1]) if len(row) > 1 else '',
                'reach': parse_int(row[2]) if len(row) > 2 else 0,
                'engagementRate': parse_float(row[3]) if len(row) > 3 else 0,
                'medianReach': parse_int(row[4]) if len(row) > 4 else 0,
                'medianER': parse_float(row[5]) if len(row) > 5 else 0,
                'quadrant': str(row[6]) if len(row) > 6 else '',
                'topic': str(row[7]) if len(row) > 7 else '',
                'actionType': str(row[8]) if len(row) > 8 else '',
                'contentPreview': str(row[9]) if len(row) > 9 else '',
                'permalink': str(row[10]) if len(row) > 10 else ''
            })
        elif current_section == 'weekly' and len(row) >= 5:
            weekly_trends.append({
                'weekRange': str(row[0]),
                'postCount': parse_int(row[1]) if len(row) > 1 else 0,
                'avgER': parse_float(row[2]) if len(row) > 2 else 0,
                'totalReach': parse_int(row[3]) if len(row) > 3 else 0,
                'totalEngagement': parse_int(row[4]) if len(row) > 4 else 0
            })

    return {
        'topPosts': top_posts[:100],  # Limit to 100
        'quadrantAnalysis': quadrant_analysis,
        'weeklyTrends': weekly_trends
    }


def process_ad_analytics(raw_rows):
    """è™•ç† ad_analytics è³‡æ–™ (raw 2D array)"""
    trending_posts = []
    best_combos = []
    recommendations = []
    organic_vs_paid = []
    campaigns = []
    roi_by_type = []

    current_section = None

    for row in raw_rows:
        if not row:
            continue

        first_val = str(row[0]) if row else ''

        # Detect section headers
        if 'ç†±é–€è²¼æ–‡' in first_val or 'è¿‘æœŸç†±é–€' in first_val:
            current_section = 'trending'
            continue
        elif 'æœ€ä½³çµ„åˆ' in first_val or 'æ­·å²æœ€ä½³' in first_val:
            current_section = 'combos'
            continue
        elif 'æŠ•å»£æ¨è–¦' in first_val:
            current_section = 'recommendations'
            continue
        elif 'è‡ªç„¶ vs ä»˜è²»' in first_val or 'è‡ªç„¶vsä»˜è²»' in first_val:
            current_section = 'organic_paid'
            continue
        elif 'å»£å‘Šæ´»å‹•' in first_val:
            current_section = 'campaigns'
            continue
        elif 'ROI' in first_val or 'æ•ˆç›Š' in first_val:
            current_section = 'roi'
            continue

        # Skip header rows
        if 'è²¼æ–‡ ID' in first_val or 'è­°é¡Œ' in first_val or 'é¡å‹' in first_val:
            continue

        # Skip empty rows
        if not first_val or first_val.strip() == '':
            continue

        # Parse data based on section
        if current_section == 'trending' and len(row) >= 8:
            trending_posts.append({
                'postId': str(row[0]),
                'messagePreview': str(row[1]) if len(row) > 1 else '',
                'createdTime': str(row[2]) if len(row) > 2 else '',
                'hoursSincePost': parse_int(row[3]) if len(row) > 3 else 0,
                'currentEngagement': parse_int(row[4]) if len(row) > 4 else 0,
                'reach': parse_int(row[5]) if len(row) > 5 else 0,
                'engagementPerHour': parse_float(row[6]) if len(row) > 6 else 0,
                'engagementRate': parse_float(row[7]) if len(row) > 7 else 0
            })
        elif current_section == 'combos' and len(row) >= 7:
            best_combos.append({
                'issueTopic': str(row[0]),
                'formatType': str(row[1]) if len(row) > 1 else '',
                'timeSlot': str(row[2]) if len(row) > 2 else '',
                'dayName': str(row[3]) if len(row) > 3 else '',
                'postCount': parse_int(row[4]) if len(row) > 4 else 0,
                'avgER': parse_float(row[5]) if len(row) > 5 else 0,
                'highPerformers': parse_int(row[6]) if len(row) > 6 else 0
            })
        elif current_section == 'recommendations' and len(row) >= 13:
            recommendations.append({
                'postId': str(row[0]),
                'createdTime': str(row[1]) if len(row) > 1 else '',
                'adRecommendation': str(row[2]) if len(row) > 2 else '',
                'adPotentialScore': parse_int(row[3]) if len(row) > 3 else 0,
                'performanceTier': str(row[4]) if len(row) > 4 else '',
                'formatType': str(row[5]) if len(row) > 5 else '',
                'issueTopic': str(row[6]) if len(row) > 6 else '',
                'breakdown': {
                    'engagementRateScore': parse_float(row[7]) if len(row) > 7 else 0,
                    'shareRateScore': parse_float(row[8]) if len(row) > 8 else 0,
                    'commentRateScore': parse_float(row[9]) if len(row) > 9 else 0,
                    'topicFactor': parse_float(row[10]) if len(row) > 10 else 1,
                    'timeFactor': parse_float(row[11]) if len(row) > 11 else 1
                },
                'permalinkUrl': str(row[12]) if len(row) > 12 else ''
            })
        elif current_section == 'organic_paid' and len(row) >= 8:
            organic_vs_paid.append({
                'type': 'paid' if 'å»£å‘Š' in str(row[0]) or 'paid' in str(row[0]).lower() else 'organic',
                'postCount': parse_int(row[1]) if len(row) > 1 else 0,
                'avgER': parse_float(row[2]) if len(row) > 2 else 0,
                'avgShareRate': parse_float(row[3]) if len(row) > 3 else 0,
                'avgCommentRate': parse_float(row[4]) if len(row) > 4 else 0,
                'avgCTR': parse_float(row[5]) if len(row) > 5 else 0,
                'totalReach': parse_int(row[6]) if len(row) > 6 else 0,
                'totalEngagement': parse_int(row[7]) if len(row) > 7 else 0
            })

    return {
        'trendingPosts': trending_posts,
        'bestCombos': best_combos,
        'recommendations': recommendations[:50],  # Limit to 50
        'organicVsPaid': organic_vs_paid,
        'campaigns': campaigns,
        'roiByType': roi_by_type
    }


def main():
    print('GCAA ç¤¾ç¾¤åˆ†æ - è³‡æ–™åŒæ­¥é–‹å§‹')
    print(f'Service Account: {SERVICE_ACCOUNT_FILE}')
    print(f'Spreadsheet ID: {SPREADSHEET_ID}')

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # é€£æ¥ Google Sheets
    print('\né€£æ¥ Google Sheets...')
    service = get_sheets_service()

    # ===== 1. è®€å– raw_post_insights =====
    print('\nè®€å– raw_post_insights...')
    raw_insights = fetch_sheet_data(service, SHEETS['raw_insights'])
    print(f'  - {len(raw_insights)} ç­†è²¼æ–‡')

    # è™•ç†è³‡æ–™
    print('\nè™•ç†è³‡æ–™...')
    posts = process_insights_data(raw_insights)
    print(f'  - è™•ç†å¾Œ: {len(posts)} ç­†è²¼æ–‡')

    # ç”Ÿæˆèšåˆè³‡æ–™
    daily = generate_daily_data(posts)
    print(f'  - æ¯æ—¥è³‡æ–™: {len(daily)} å¤©')

    stats = generate_stats(posts)
    print(f'  - è¡Œå‹•é¡å‹: {len(stats["byActionType"])} ç¨®')
    print(f'  - è­°é¡Œ: {len(stats["byTopic"])} ç¨®')

    # ===== 2. è®€å– content_analysis =====
    print('\nè®€å– content_analysis...')
    try:
        raw_content = fetch_sheet_raw(service, SHEETS['content_analysis'])
        print(f'  - åŸå§‹è³‡æ–™: {len(raw_content)} åˆ—')
        content_analysis = process_content_analysis(raw_content)
        print(f'  - è¡Œå‹•é¡å‹: {len(content_analysis["byActionType"])} ç¨®')
        print(f'  - è­°é¡Œ: {len(content_analysis["byTopic"])} ç¨®')
        print(f'  - äº¤å‰åˆ†æ: {len(content_analysis["crossAnalysis"])} çµ„')
    except Exception as e:
        print(f'  - è®€å–å¤±æ•—: {e}')
        import traceback
        traceback.print_exc()
        content_analysis = {'byActionType': [], 'byTopic': [], 'crossAnalysis': []}

    # ===== 3. è®€å– posts_performance =====
    print('\nè®€å– posts_performance...')
    try:
        raw_performance = fetch_sheet_raw(service, SHEETS['posts_performance'])
        print(f'  - åŸå§‹è³‡æ–™: {len(raw_performance)} åˆ—')
        posts_performance = process_posts_performance(raw_performance)
        print(f'  - Top è²¼æ–‡: {len(posts_performance["topPosts"])} ç­†')
        print(f'  - è±¡é™åˆ†æ: {len(posts_performance["quadrantAnalysis"])} ç­†')
        print(f'  - é€±è¶¨å‹¢: {len(posts_performance["weeklyTrends"])} é€±')
    except Exception as e:
        print(f'  - è®€å–å¤±æ•—: {e}')
        import traceback
        traceback.print_exc()
        posts_performance = {'topPosts': [], 'quadrantAnalysis': [], 'weeklyTrends': []}

    # ===== 4. è®€å– ad_analytics =====
    print('\nè®€å– ad_analytics...')
    try:
        raw_ads = fetch_sheet_raw(service, SHEETS['ad_analytics'])
        print(f'  - åŸå§‹è³‡æ–™: {len(raw_ads)} åˆ—')
        ad_analytics = process_ad_analytics(raw_ads)
        print(f'  - ç†±é–€è²¼æ–‡: {len(ad_analytics["trendingPosts"])} ç­†')
        print(f'  - æœ€ä½³çµ„åˆ: {len(ad_analytics["bestCombos"])} çµ„')
        print(f'  - æŠ•å»£æ¨è–¦: {len(ad_analytics["recommendations"])} ç­†')
        print(f'  - è‡ªç„¶vsä»˜è²»: {len(ad_analytics["organicVsPaid"])} çµ„')
    except Exception as e:
        print(f'  - è®€å–å¤±æ•—: {e}')
        import traceback
        traceback.print_exc()
        ad_analytics = {
            'trendingPosts': [], 'bestCombos': [], 'recommendations': [],
            'organicVsPaid': [], 'campaigns': [], 'roiByType': []
        }

    # ===== å¯«å…¥ JSON æª”æ¡ˆ =====
    print('\nå¯«å…¥ JSON æª”æ¡ˆ...')

    # 1. posts.json
    posts_file = os.path.join(OUTPUT_DIR, 'posts.json')
    with open(posts_file, 'w', encoding='utf-8') as f:
        json.dump(posts, f, ensure_ascii=False, indent=2)
    print(f'  - {posts_file}')

    # 2. daily.json
    daily_file = os.path.join(OUTPUT_DIR, 'daily.json')
    with open(daily_file, 'w', encoding='utf-8') as f:
        json.dump(daily, f, ensure_ascii=False, indent=2)
    print(f'  - {daily_file}')

    # 3. stats.json
    stats_file = os.path.join(OUTPUT_DIR, 'stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f'  - {stats_file}')

    # 4. content-analysis.json (NEW)
    content_file = os.path.join(OUTPUT_DIR, 'content-analysis.json')
    with open(content_file, 'w', encoding='utf-8') as f:
        json.dump(content_analysis, f, ensure_ascii=False, indent=2)
    print(f'  - {content_file}')

    # 5. posts-performance.json (NEW)
    performance_file = os.path.join(OUTPUT_DIR, 'posts-performance.json')
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(posts_performance, f, ensure_ascii=False, indent=2)
    print(f'  - {performance_file}')

    # 6. ad-analytics.json (NEW)
    ads_file = os.path.join(OUTPUT_DIR, 'ad-analytics.json')
    with open(ads_file, 'w', encoding='utf-8') as f:
        json.dump(ad_analytics, f, ensure_ascii=False, indent=2)
    print(f'  - {ads_file}')

    print('\nåŒæ­¥å®Œæˆ!')
    print(f'è³‡æ–™æ›´æ–°æ™‚é–“: {stats["lastUpdated"]}')

if __name__ == '__main__':
    main()
